{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "In this notebook, we attempt to a train a model to generate a embedding based on keystroke dyanmics based on facenets triplet loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "from dataprep import generate_pair_features\n",
    "from keyprint.model import *\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=\"/tf/data/preped/greyc_web/\"\n",
    "TF_LOG_DIR=\"/tf/logs\"\n",
    "!mkdir -p {TF_LOG_DIR}\n",
    "ENCODING_DIM = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Preparing the data\n",
    "In section, we load the prepared Greyc dataset and prepare it for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata frame\n",
    "meta_df = pd.read_feather(f\"{DATA_DIR}/meta.feather\")\n",
    "userids = meta_df.userid\n",
    "\n",
    "# load keystroke features\n",
    "with open(f\"{DATA_DIR}/keystroke.npz\", \"rb\") as f:\n",
    "    keystroke_features = np.load(f, allow_pickle=True)[\"keystroke\"]\n",
    "N_FEATURES = keystroke_features[0].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Pairs\n",
    "Since our model will take 2 keystoke features and compare them, we generate pairs of keystroke features and label them 1 if they are from the same user, 0 if they are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.13 s, sys: 4.43 s, total: 7.56 s\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feature_pairs, labels = generate_pair_features(keystroke_features, meta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Test Train\n",
    "Split the dataset between test and train subsets randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, valid_features, train_labels, valid_labels = \\\n",
    "    train_test_split(feature_pairs, labels, shuffle=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset\n",
    "Since we are training on a neural network, we have to normalise our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 284 ms, sys: 87.8 ms, total: 372 ms\n",
      "Wall time: 369 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_shape = train_features.shape\n",
    "valid_shape = valid_features.shape\n",
    "\n",
    "scaler.fit(train_features.reshape((train_shape[0], -1)))\n",
    "\n",
    "    \n",
    "train_features = scaler.transform(train_features.reshape((train_shape[0], -1))).reshape(train_shape)\n",
    "valid_features = scaler.transform(valid_features.reshape((valid_shape[0], -1))).reshape(valid_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpacking the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_refs, train_evals =  train_features[:, 0], train_features[:, 1]\n",
    "valid_refs, valid_evals = valid_features[:, 0], valid_features[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "The model be built is a siamese network composed of two components:\n",
    "- encoder that transform the keystroke features to embedding\n",
    "- evaluator model that computes the predicted distance between embeddings\n",
    "- objective (define by loss function) that ensures that embedding produced can be used to identify the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder is a simple 1D CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 64, 5)]           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 64, 8)             288       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 64, 8)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 32, 8)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 32, 16)            400       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 32, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 16, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16, 32)            1568      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 16, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 4,896\n",
      "Trainable params: 4,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = build_encoder(\n",
    "              n_input_dim=N_FEATURES,\n",
    "              n_encoding_dim=ENCODING_DIM,\n",
    "              n_conv_block=3,\n",
    "              n_conv_layers=[1, 1, 1],\n",
    "              n_conv_filters=[8, 16, 32],\n",
    "              conv_filter_size=[7, 3, 3],\n",
    "              n_dense_layers=2,\n",
    "              n_dense_units=32,\n",
    "              activation=(lambda: Activation(\"selu\")),\n",
    "              batch_norm=False,\n",
    "              l2_lambda=0,\n",
    "              dropout_prob=0)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator model\n",
    "The evaluator uses the encoder to compute the distance between predicted embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, None, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, None, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_6 (Model)                 (None, 16)           4896        input_11[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               ()                   0           model_6[1][0]                    \n",
      "                                                                 model_6[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,896\n",
      "Trainable params: 4,896\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build(N_FEATURES, encoder)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective & Loss Function\n",
    "The objective of this model is to drive the distance of embeddings of different users as apart (up to margin), while makeing the distance of the embeddings of the same user as close as possible\n",
    "\n",
    "As such, the model utilies **constrastive loss** as its loss function\n",
    "![Loss Function](https://cdn-images-1.medium.com/max/1600/1*Uo5IovRsjW86b-vCBZGRvg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Setup is complete, we can finally train the model and tune its hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24170 samples, validate on 6043 samples\n",
      "Epoch 1/10\n",
      "24170/24170 [==============================] - 3s 125us/sample - loss: 155.1871 - val_loss: 2090935.3441\n",
      "Epoch 2/10\n",
      "24170/24170 [==============================] - 1s 43us/sample - loss: 60.4399 - val_loss: 1668312.6964\n",
      "Epoch 3/10\n",
      "24170/24170 [==============================] - 1s 42us/sample - loss: 52.2511 - val_loss: 1296700.0483\n",
      "Epoch 4/10\n",
      "24170/24170 [==============================] - 1s 43us/sample - loss: 47.3033 - val_loss: 964780.6862\n",
      "Epoch 5/10\n",
      "24170/24170 [==============================] - 1s 43us/sample - loss: 45.5588 - val_loss: 956452.6614\n",
      "Epoch 6/10\n",
      "24170/24170 [==============================] - 1s 43us/sample - loss: 44.7548 - val_loss: 686980.6417\n",
      "Epoch 7/10\n",
      "24170/24170 [==============================] - 1s 43us/sample - loss: 44.4758 - val_loss: 578251.6922\n",
      "Epoch 8/10\n",
      "24170/24170 [==============================] - 1s 43us/sample - loss: 43.7953 - val_loss: 480669.3028\n",
      "Epoch 9/10\n",
      "24170/24170 [==============================] - 1s 44us/sample - loss: 43.2864 - val_loss: 551651.0785\n",
      "Epoch 10/10\n",
      "24170/24170 [==============================] - 1s 44us/sample - loss: 43.3870 - val_loss: 437563.7662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc942541080>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "model.compile(loss=contrastive_loss,\n",
    "              optimizer=optimizer,\n",
    "             accuracy\n",
    "\n",
    "# train the model\n",
    "model.fit([train_refs, train_evals], train_labels,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=([valid_refs, valid_evals], valid_labels),\n",
    "          callbacks=[TensorBoard(TF_LOG_DIR),\n",
    "                     ReduceLROnPlateau(factor=0.1, \n",
    "                                       patience=10,\n",
    "                                       verbose=1,\n",
    "                                       min_lr=1e-9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
